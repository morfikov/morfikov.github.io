<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>debian on Morfitronik</title>
    <link>https://morfikov.github.io/tags/debian/</link>
    <description>Recent content in debian on Morfitronik</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>pl-PL</language>
    <lastBuildDate>Tue, 13 Dec 2022 13:03:00 +0100</lastBuildDate><atom:link href="https://morfikov.github.io/tags/debian/feed.xml" rel="self" type="application/rss+xml" />
    <item>
	  <author>Mikhail Morfikov</author>
      <title>Zablokowanie możliwości ładowania modułów kernela na Debian linux</title>
      <link>https://morfikov.github.io/post/zablokowanie-mozliwosci-ladowania-modulow-kernela-na-debian-linux/</link>
      <pubDate>Tue, 13 Dec 2022 13:03:00 +0100</pubDate>
      
      <guid>https://morfikov.github.io/post/zablokowanie-mozliwosci-ladowania-modulow-kernela-na-debian-linux/</guid>
      <description>&lt;p&gt;Kernele oferowane przez różne dystrybucje linux&#39;a, np. Debian czy Ubuntu, są tak budowane by
możliwie jak największa ilość sprzętu na takim jądrze operacyjnym nam zadziałała bez zbędnego
przerabiania systemu. Takie podejście sprawia, że nowo nabyty przez nas sprzęt możemy od razu
podłączyć do komputera, a stosowne moduły po rozpoznaniu urządzenia zostaną załadowane do pamięci
operacyjnej, przez co my będziemy mogli wejść w interakcję z takim kawałkiem elektroniki. Problem w
tym, że kernel ma bardzo dużo tych modłów. Dużo modułów, to więcej kodu, a więcej kodu to więcej
błędów, które na poziomie jądra mogą być bardzo opłakane w skutkach. Z zagrożeniem, jakie niosą
moduły zewnętrzne (te spoza drzewa kernela linux), można sobie poradzić &lt;a href=&#34;https://morfikov.github.io/post/jak-dodac-wlasne-klucze-dla-secure-boot-do-firmware-efi-uefi-pod-linux/#podpisywanie-kernela&#34;&gt;podpisując kernel kluczami
od firmware EFI/UEFI&lt;/a&gt;. W takim przypadku załadowanie niepodpisanego modułu już się nie powiedzie.
Niemniej jednak, dystrybucyjne kernele mają całą masę modułów do różnorakiego sprzętu, które na
etapie budowania kernela są podpisywane, co otwiera im drogę do bycia załadowanymi w naszym
systemie nawet jeśli nie posiadamy urządzeń, które by użytek z tych modułów robiły. Przydałoby się
zatem zabezpieczyć możliwość ładowania modułów kernela w taki sposób, by jedynie administrator
systemu miał możliwość określenia jakie moduły i na którym etapie pracy systemu mogą one zostać
załadowane.&lt;/p&gt;</description>
    </item>
    
    <item>
	  <author>Mikhail Morfikov</author>
      <title>Oblivious DoH (ODoH) z dnscrypt-proxy na Debian Linux</title>
      <link>https://morfikov.github.io/post/oblivious-doh-odoh-z-dnscrypt-proxy-na-debian-linux/</link>
      <pubDate>Mon, 21 Nov 2022 17:42:00 +0100</pubDate>
      
      <guid>https://morfikov.github.io/post/oblivious-doh-odoh-z-dnscrypt-proxy-na-debian-linux/</guid>
      <description>&lt;p&gt;Parę miesięcy temu natrafiłem na taki oto &lt;a href=&#34;https://blog.cloudflare.com/oblivious-dns/&#34;&gt;artykuł na blogu Cloudflare&lt;/a&gt;, który poświęcony jest
poprawie prywatności wykonywanych przez klientów zapytań DNS za sprawą wykorzystania mechanizmu
zwanego Oblivious DoH (ODoH). Oczywiście postanowiłem już wtedy zbadać czym ten ODoH jest ale
standardowe narzędzia dostępne w linux&#39;ie (Debian/Ubuntu) jeszcze (przynajmniej wtedy) nie dojrzały
do obsługi ODoH. Obecnie już jest trochę lepiej ale Oblivious DoH wciąż jest w fazie
eksperymentów (&lt;a href=&#34;https://datatracker.ietf.org/doc/html/rfc9230&#34;&gt;RFC9230&lt;/a&gt;). Póki co, jedynym znanym mi narzędziem, które posiada wsparcie dla
ODoH jest &lt;a href=&#34;https://dnscrypt.info/&#34;&gt;dnscrypt-proxy&lt;/a&gt;, którego instalację i konfigurację do współpracy z &lt;code&gt;dnsmasq&lt;/code&gt;
już &lt;a href=&#34;https://morfikov.github.io/post/szyfrowany-dns-z-dnscrypt-proxy-i-dnsmasq-na-debian-linux/&#34;&gt;jakiś czas temu opisywałem&lt;/a&gt;. Mając wsparcie dla ODoH w &lt;code&gt;dnscrypt-proxy&lt;/code&gt; możemy pokusić się
o wdrożenie Oblivious DoH w swoim systemie i sprawdzić czy faktycznie taki zabieg przyczyni się do
poprawy prywatności wykonywanych przez nas zapytań DNS i właśnie temu zagadnieniu będzie poświęcony
niniejszy wpis.&lt;/p&gt;</description>
    </item>
    
    <item>
	  <author>Mikhail Morfikov</author>
      <title>Rekalibracja baterii w laptopach Lenovo ThinkPad pod linux</title>
      <link>https://morfikov.github.io/post/rekalibracja-baterii-w-laptopach-lenovo-thinkpad-pod-linux/</link>
      <pubDate>Tue, 01 Feb 2022 18:09:00 +0100</pubDate>
      
      <guid>https://morfikov.github.io/post/rekalibracja-baterii-w-laptopach-lenovo-thinkpad-pod-linux/</guid>
      <description>&lt;p&gt;Jakiś już czas temu opisywałem temat &lt;a href=&#34;https://morfikov.github.io/post/jak-ograniczyc-ladowanie-baterii-w-laptopie-thinkpad-t430/&#34;&gt;konfiguracji progów ładowania baterii w laptopie Lenovo
ThinkPad T430&lt;/a&gt;, tak by przedłużyć parokrotnie jej żywotność. Niemniej jednak, ustawienie na
dłuższy czas progów 30-40% nie pozostaje bez wpływu na pracę maszyny. Może i same baterie
litowo-jonowe nie posiadają efektu pamięci ale elektronika, która tymi bateriami zarządza, już tę
pamięć może posiadać. Chodzi generalnie o to, że brak pełnych cyklów baterii (rozładowanie do 0% i
ładowanie do 100%) może powodować różne błędy w ocenie ile faktycznie tego ładunku w takim
akumulatorze pozostało. Efektem tych błędów są wskazania sugerujące, że nasz laptop może jeszcze
popracować, np. 20-30 minut dłużej, gdy stan faktyczny na to nie pozwala, co skończyć może się
utratą niezapisanych danych w skutek nagłego odcięcia zasilania. Dlatego też jeśli wykorzystujemy
jedynie pewien zakres pojemności baterii w laptopie, to co pewien czas (co 30-50 takich niepełnych
cykli albo raz na 2 miesiące) przydałoby się przeprowadzić w laptopie proces rekalibracji baterii
(a raczej rekalibrację jej kontrolera/elektroniki), tak by ewentualne straty pojemności zostały
uwzględnione w szacunkach systemu. Zabieg rekalibracji baterii można bez większego problemu
przeprowadzić z poziomu dowolnej dystrybucji linux&#39;a, np. Debian/Ubuntu, i tym zagadnieniem się
zajmiemy w niniejszym artykule.&lt;/p&gt;</description>
    </item>
    
    <item>
	  <author>Mikhail Morfikov</author>
      <title>Chroot do 32-bit systemu ARM z poziomu 64-bit linux&#39;owego hosta</title>
      <link>https://morfikov.github.io/post/chroot-do-32-bit-systemu-arm-z-poziomu-64-bit-linuxowego-hosta/</link>
      <pubDate>Sun, 07 Nov 2021 10:20:00 +0100</pubDate>
      
      <guid>https://morfikov.github.io/post/chroot-do-32-bit-systemu-arm-z-poziomu-64-bit-linuxowego-hosta/</guid>
      <description>&lt;p&gt;Eksperymentując ostatnio z moją maszynką Raspberry Pi 4B, zaszła potrzeba, by zejść do systemu
RasPiOS/Raspbian przy pomocy mechanizmu chroot. Problem w tym, że system wyrzuca  komunikat:
&lt;code&gt;chroot: failed to run command ‘/bin/bash’: Exec format error&lt;/code&gt; . Niby wszystko jest na swoim
miejscu ale ta widoczna wyżej wiadomość nie chce zniknąć uniemożliwiając tym samym dalszą zabawę z
RPI. Okazało się, że winna jest tutaj architektura CPU. Mój laptop działa pod kontrolą 64-bitowego
Intel&#39;owskiego procesora (x64, x86-64, AMD64), na którym uruchomiony jest również 64-bitowy Debian
linux. Z kolei Raspberry Pi ma 64-bitowy procesor ARM (ARMv8-A) działający pod kontrolą 32-bitowego
systemu operacyjnego. Te dość spore rozbieżności sprawiają, że nie damy rady skorzystać z chroot,
przynajmniej nie bez zaprzęgnięcia do tego celu emulatora QEMU.&lt;/p&gt;</description>
    </item>
    
    <item>
	  <author>Mikhail Morfikov</author>
      <title>Drukowanie zeskanowanych dokumentów tekstowych na drukarce laserowej</title>
      <link>https://morfikov.github.io/post/drukowanie-zeskanowanych-dokumentow-tekstowych-na-drukarce-laserowej/</link>
      <pubDate>Tue, 26 Oct 2021 19:57:00 +0200</pubDate>
      
      <guid>https://morfikov.github.io/post/drukowanie-zeskanowanych-dokumentow-tekstowych-na-drukarce-laserowej/</guid>
      <description>&lt;p&gt;Posiadacze monochromatycznych (czarnobiałych) drukarek laserowych zapewne spotkali się z problemem
drukowania kolorowych dokumentów na tego typu urządzeniach. Nie chodzi tutaj o drukowanie obrazków
czy innych elementów graficznych ale o wydrukowanie, np. zeskanowanej książki. Ostatnio przyszło mi
wydrukować dokumentację techniczną dość starego urządzenia. Problem w tym, że nie był to zwykły
kawałek książki, a jedynie jej skany, z których ktoś postanowił zrobić plik PDF . Takiego
dokumentu &amp;quot;tekstowego&amp;quot; za bardzo nie da się wydrukować na drukarce laserowej, przynajmniej nie bez
pchania się w ogromne koszty. Dla przykładu, w miejsce pożółkniętej kartki ze skanu, taka drukarka
wstawi jakiś odcień szarości i w ten sposób zadrukuje tą szarością całą stronę marnując przy tym
niesamowite ilości toneru, za który my będziemy musieli później zapłacić, co czyni cały proces
drukowania zeskanowanych dokumentów bardzo kosztownym. Postanowiłem jednak znaleźć jakiś sposób, by
tę dokumentację wydrukować, choć trzeba było pierw zająć się samymi skanami, tj. doprowadzić je do
stanu, w którym można by mówić o ewentualnym ich wydrukowaniu. Okazało się, że nie jest to jakoś
specjalnie trudne zadanie, zwłaszcza, gdy na linux zaprzęgnie się do tego celu ImageMagick.&lt;/p&gt;</description>
    </item>
    
    <item>
	  <author>Mikhail Morfikov</author>
      <title>Jak wyłączyć mapowanie adresów IPv4 na IPv6 w Debian linux</title>
      <link>https://morfikov.github.io/post/jak-wylaczyc-mapowane-adresow-ipv4-na-ipv6-w-debian-linux/</link>
      <pubDate>Thu, 14 Oct 2021 23:52:00 +0200</pubDate>
      
      <guid>https://morfikov.github.io/post/jak-wylaczyc-mapowane-adresow-ipv4-na-ipv6-w-debian-linux/</guid>
      <description>&lt;p&gt;Przeglądając ostatnio połączenia w swoim telefonie z Androidem, natknąłem się na wpisy mające w
swoich adresach źródłowych ustawione IP takie, jak &lt;code&gt;::ffff:192.168.1.188&lt;/code&gt; . Podobnie sprawa wygląda
w przypadku adresów docelowych tego samego połączenia, tj. widnieje tam np. &lt;code&gt;::ffff:216.58.215.1&lt;/code&gt; .
Na pierwszy rzut oka taka konstrukcja adresu IP przypomina IPv6, przynajmniej jej pierwsza część,
tj. &lt;code&gt;::ffff:&lt;/code&gt; , natomiast drugi kawałek już bardzo przypomina adres IPv4. Okazuje się, że za taki
format adresów IP odpowiada &lt;a href=&#34;https://en.wikipedia.org/wiki/IPv6#IPv4-mapped_IPv6_addresses&#34;&gt;mechanizm mapowania adresów IPv4 na adresy IPv6&lt;/a&gt;. Nie jest to
jednak tożsame z &lt;a href=&#34;https://morfikov.github.io/post/implementacja-protokolu-ipv6-za-pomoca-tunelu-6to4/&#34;&gt;6to4&lt;/a&gt; czy &lt;a href=&#34;https://morfikov.github.io/post/konfiguracja-tunelu-6in4-w-openwrt-ipv6/&#34;&gt;6in4&lt;/a&gt;, gdzie host ze stałym publicznym adresem IPv4 jest w
stanie komunikować się z hostami nadającymi po IPv6. W przypadku mapowania adresów IPv4 na IPv6,
połączenia ze zdalnymi hostami mogą odbywać się zarówno po IPv4 jak i IPv6, a to z którego z tych
protokołów nasz linux skorzysta zależy od tego czy taka maszyna dysponuje przydzielonym adresem
IPv6. Jeśli ISP nie zapewnia nam adresu IPv6, to wtedy pakiety sieciowe będą przesyłane z
wykorzystaniem protokołu IPv4. Wciąż jednak te mieszane konstrukcje adresów IP będą widoczne na
wykazie połączeń w &lt;code&gt;netstat&lt;/code&gt;/&lt;code&gt;ss&lt;/code&gt; . Okazuje się jednak, że ten mechanizm mapowania adresów może
powodować szereg problemów związanych z bezpieczeństwem systemu. Dlatego też tam gdzie to możliwe
przydałoby się go wyłączyć.&lt;/p&gt;</description>
    </item>
    
    <item>
	  <author>Mikhail Morfikov</author>
      <title>Jak zaktualizować firmware custom ROM&#39;ów w smartfonach Xiaomi</title>
      <link>https://morfikov.github.io/post/jak-zaktualizowac-firmware-custom-rom-w-smartfonach-xiaomi/</link>
      <pubDate>Thu, 30 Sep 2021 02:43:00 +0200</pubDate>
      
      <guid>https://morfikov.github.io/post/jak-zaktualizowac-firmware-custom-rom-w-smartfonach-xiaomi/</guid>
      <description>&lt;p&gt;Te bardziej szanujące się marki produkujące smartfony zwykle zapewniają wsparcie dla swoich
urządzeń przez co najmniej dwa lata (a czasem nawet i dłużej) od momentu ich wypuszczenia na rynek.
Po wgraniu sobie alternatywnego ROM&#39;u na nasz telefon, oprogramowanie w nim może być aktualizowane
przez opiekuna czy dewelopera takiego ROM&#39;u znacznie dłużej niż producent przewidział. W ten sposób
nie musimy wydawać pieniążków na nowy sprzęt, oczywiście zakładając, że mu nic nie dolega, np. pod
względem wydajności, czy też ewentualnie nie zużył on się nam jakoś bardziej podczas eksploatacji.
Jedną rzeczą, o której posiadacze smartfonów z Androidem zapominają po wgraniu custom ROM&#39;ów na
bazie AOSP/LineageOS, to fakt, że o ile ROM faktycznie dostaje aktualizacje czy to bezpieczeństwa,
czy też upgrade do nowszej wersji Androida, o tyle sam firmware zwykle pozostaje nietknięty. W
przypadku mojego modelu smartfona Redmi 9, Xiaomi od czasu do czasu wypuszcza aktualizacje firmware
do tego urządzenia i przydałoby się ten firmware co jakiś czas zaktualizować. Na szczęście nie
trzeba w tym celu powracać do stock&#39;owego oprogramowania, a cały proces możemy przeprowadzić z
poziomu dowolnej dystrybucji linux&#39;a.&lt;/p&gt;</description>
    </item>
    
    <item>
	  <author>Mikhail Morfikov</author>
      <title>Kopia zapasowa danych smartfona z Androidem (OAndBackupX, Syncthing)</title>
      <link>https://morfikov.github.io/post/kopia-zapasowa-danych-smartfona-z-androidem-oandbackupx-syncthing/</link>
      <pubDate>Sun, 26 Sep 2021 16:44:00 +0200</pubDate>
      
      <guid>https://morfikov.github.io/post/kopia-zapasowa-danych-smartfona-z-androidem-oandbackupx-syncthing/</guid>
      <description>&lt;p&gt;Jak to zwykło się mawiać w świecie IT: ludzie dzielą się na te osobniki, które robią backup i
te, co backup robić będą. Jakiś czas temu pochyliłem się nad zagadnieniem &lt;a href=&#34;https://morfikov.github.io/post/czy-smartfon-z-androidem-bez-google-apps-services-ma-sens/&#34;&gt;czy smartfon z
Androidem bez Google Play Services ma sens&lt;/a&gt;. Poruszyłem tam problem tworzenia kopi zapasowej
danych użytkownika zgromadzonych w telefonie. W tym podlinkowanym artykule zostało przestawione jak
przy pomocy TWRP dokonać pełnego backupu całego systemu urządzenia i o ile pod względem technicznym
takie podejście było jak najbardziej w pełni do zaakceptowania, to jednak to rozwiązanie miało dość
istotną wadę, tj. na czas backup&#39;u trzeba było wyłączyć smartfon. Parę dni temu natrafiłem
na &lt;a href=&#34;https://github.com/machiav3lli/oandbackupx&#34;&gt;narzędzia OAndBackupX&lt;/a&gt;, które jest w stanie zrobić kopię zapasową wszystkich zainstalowanych
w Androidzie aplikacji (oraz ich ustawień), wliczając w to nawet appki systemowe. OAndBackupX jest
o tyle lepszym rozwiązaniem w stosunku do TWRP, że można przy jego pomocy robić kopię danych
pojedynczych aplikacji, a nie od razu całego ROM&#39;u, co nie tylko jest czasochłonne ale również
zjada sporo miejsca na dysku komputera, czy gdzie ten backup zamierzamy przechowywać. Niestety
OAndBackupX wymaga uprawnień root, zatem na stock&#39;owym ROM&#39;ie producenta naszego telefonu nie damy
rady z tego narzędzia skorzystać. Jeśli jednak mamy alternatywny ROM na bazie AOSP/LineageOS, to
przydałoby się rzucić okiem na ten kawałek oprogramowania, bo zdaje się ono być wielce użyteczne,
zwłaszcza w przypadku osób mojego pokroju, czyli linux&#39;iarzy, którzy o backup swoich danych chcą
zatroszczyć się tylko i wyłącznie we własnym zakresie, zaprzęgając do pracy choćby &lt;a href=&#34;https://github.com/syncthing/syncthing&#34;&gt;Syncthing&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
	  <author>Mikhail Morfikov</author>
      <title>Kontrola podświetlenia klawiatury via UDEV (backlight)</title>
      <link>https://morfikov.github.io/post/kontrola-podswietlenia-klawiatury-via-udev-backlight/</link>
      <pubDate>Sat, 04 Sep 2021 12:36:00 +0200</pubDate>
      
      <guid>https://morfikov.github.io/post/kontrola-podswietlenia-klawiatury-via-udev-backlight/</guid>
      <description>&lt;p&gt;Ostatnio na &lt;a href=&#34;https://forum.linuxmint.pl/showthread.php?tid=1739&amp;amp;pid=13156&#34;&gt;polskim forum linux mint pojawił się wątek&lt;/a&gt;, w którym jeden z użytkowników miał
problem z ogarnięciem podświetlania klawiatury. Chodzi generalnie o start komputera z włączonym
backlight&#39;em klawiatury, przez co trzeba to podświetlenie manualnie wyłączać za każdym razem po
uruchomieniu się systemu. W przypadku mojego laptopa Lenovo ThinkPad T430, taka sytuacja co prawda
nie występuje i system uruchamia się naturalnie ze zgaszoną klawiaturą. Niemniej jednak,
zainteresował mnie ten problem tyle, że w drugą stronę -- chodzi o możliwość startu systemu z
włączonym podświetleniem klawiatury, bo ustawienia BIOS/EFI/UEFI mojego laptopa tego aspektu pracy
komputera nie są w stanie w żaden sposób skonfigurować. Okazało się, że zaimplementowanie tego typu
funkcjonalności nie jest jakoś specjalnie trudne i można bez większego trudu ogarnąć backlight
klawiatury przy pomocy prostej reguły dla UDEV&#39;a.&lt;/p&gt;</description>
    </item>
    
    <item>
	  <author>Mikhail Morfikov</author>
      <title>Jak odblokować bootloader w Xiaomi Redmi 9 (galahad/lancelot)</title>
      <link>https://morfikov.github.io/post/jak-odblokowac-bootloader-w-xiaomi-redmi-9-galahad-lancelot/</link>
      <pubDate>Fri, 27 Aug 2021 22:26:00 +0200</pubDate>
      
      <guid>https://morfikov.github.io/post/jak-odblokowac-bootloader-w-xiaomi-redmi-9-galahad-lancelot/</guid>
      <description>&lt;p&gt;Jakiś czas temu wpadł w moje łapki smartfon Xiaomi Redmi 9 (galahad albo lancelot, bo w różnych
częściach systemu jest to inaczej określone), który miał preinstalowanego Androida 10 oraz MIUI 11.
Przez parę miesięcy używania telefonu, dostał on dwa albo trzy większe update całego ROM&#39;u,
wliczając w to aktualizację MIUI do 12.0.1 ze stanem zabezpieczeń na dzień 2021-01-05. Zatem
ostatnia aktualizacja zabezpieczeń tego telefonu miała miejsce zaraz na początku Stycznia. Od tego
czasu cisza. Niby w przypadku tego modelu telefonu aktualizacje miały być wydawane &lt;a href=&#34;https://www.mi.com/global/service/support/security-update-1.html&#34;&gt;co trzy
miesiące do roku 2023&lt;/a&gt; ale najwyraźniej coś jest nie tak i urządzenie od ponad pół roku nie
dostało żadnych aktualizacji. Niby pod tym linkiem można wyczytać informację, że planowana jest
aktualizacja do Androida 11 ale prawdę mówiąc jestem nieco zawiedziony opieszałością Xiaomi. Tak
się złożyło, że przez przypadek trafiłem w &lt;a href=&#34;https://forum.xda-developers.com/f/redmi-9-poco-m2-roms-kernels-recoveries-dev.11175/&#34;&gt;to miejsce na forum XDA&lt;/a&gt;, gdzie z kolei znalazłem
m.in. &lt;a href=&#34;https://forum.xda-developers.com/t/rom-unofficial-11-0-pixel-plus-ui-for-redmi-9-lancelot-galahad.4243813/&#34;&gt;ten wątek&lt;/a&gt;. Zatem alternatywne ROM&#39;y na mój smartfon istnieją i tego faktu nie byłem
świadomy, bo w zeszłym roku jeszcze nic nie szło znaleźć. Postanowiłem zatem odblokować bootloader
w swoim Xiaomi Redmi 9 i spróbować wgrać na niego TWRP i jeden (a może nawet kilka) przykładowy ROM
na bazie AOSP/LineageOS. Proces odblokowania bootloader&#39;a w urządzeniach Xiaomi nie wymaga zbytnio
wysiłku i da się go przeprowadzić w całości pod linux korzystając czy to z XiaoMiTool, czy też przy
pomocy &lt;a href=&#34;https://morfikov.github.io/post/wirtualizacja-qemu-kvm-libvirt-na-debian-linux/&#34;&gt;maszyn wirtualnych na bazie QEMU/KVM&lt;/a&gt;. Ten proces nie do końca jest dla każdego taki
oczywisty, dlatego postanowiłem go dokładnie opisać.&lt;/p&gt;</description>
    </item>
    
    <item>
	  <author>Mikhail Morfikov</author>
      <title>Migracja z apt-key w Debian linux</title>
      <link>https://morfikov.github.io/post/migracja-z-apt-key-w-debian-linux/</link>
      <pubDate>Sat, 21 Aug 2021 14:35:00 +0200</pubDate>
      
      <guid>https://morfikov.github.io/post/migracja-z-apt-key-w-debian-linux/</guid>
      <description>&lt;p&gt;Z okazji wypuszczenia parę dni temu nowego Debiana, przeglądałem sobie &lt;a href=&#34;https://www.debian.org/releases/bullseye/amd64/release-notes/ch-information.en.html&#34;&gt;notki dla wydania
stabilnego&lt;/a&gt; pod kątem aktualizacji z buster (10) -&amp;gt; bullseye (11). Niby ja i tak korzystam cały
czas z unstable/experimental i zwykle jestem na bieżąco ze zmianami wprowadzanymi w tej dystrybucji
linux&#39;a ale też zawsze coś może umknąć uwadze z perspektywy wielu miesięcy czy nawet kilku lat
(a dokładnie 25 miesięcy). Sporo z tych rzeczy, które w tych podlinkowanych notatkach wyczytałem,
miałem już załatwione wcześniej ale zapomniałem rozprawić się z repozytoriami APT. Konkretnie
chodzi tutaj o odejście od &lt;code&gt;apt-key&lt;/code&gt; , czyli narzędzia, które w Debianie używane jest do dodawania
kluczy GPG do systemowego keyring&#39;a APT. Te klucze zwykle wykorzystywane są do weryfikacji sygnatur
złożonych pod zewnętrznymi repozytoriami, a że ja mam ich sporo, to musiałem się nieco zagłębić w
temat i ustalić w jaki sposób od następnego wydania Debiana (bookworm/12) będzie się te klucze GPG
od takich repozytoriów ogarniać. No i właśnie o tym będzie ten poniższy kawałek artykułu.&lt;/p&gt;</description>
    </item>
    
    <item>
	  <author>Mikhail Morfikov</author>
      <title>Analiza systemu plików EXT4 pod kątem formatowania większych dysków pod linux</title>
      <link>https://morfikov.github.io/post/analiza-systemu-plikow-ext4-pod-katem-formatowania-wiekszych-dyskow-pod-linux/</link>
      <pubDate>Sun, 25 Jul 2021 18:40:00 +0200</pubDate>
      
      <guid>https://morfikov.github.io/post/analiza-systemu-plikow-ext4-pod-katem-formatowania-wiekszych-dyskow-pod-linux/</guid>
      <description>&lt;p&gt;Zapewne każdy użytkownik linux&#39;a tworzył na dysku HDD/SSD partycje sformatowane systemem plików
EXT4. Prawdopodobnie też zastanawiało nas pytanie odnośnie ilości zajmowanego miejsca przez
strukturę samego systemu plików, zwłaszcza w przypadku dysków o sporych rozmiarach (setki GiB, czy
nawet kilka TiB). Jako, że musiałem ostatnio zmigrować kolekcję filmów ze strych dysków na
jeden większy, który miał zostać podłączony pod Raspberry Pi z działającym Kodi na bazie LibreELEC,
to przy okazji postanowiłem ten dysk sformatować w taki sposób, w jaki powinno się do tego zadania
podchodzić wiedząc, że ma się do czynienia z dużym dyskiem, na którym będą przechowywane głównie
duże pliki. Celem tego artykułu jest pokazanie jakie błędy przy tworzeniu systemu plików EXT4 można
popełnić przez posiadanie niezbyt wystarczającej wiedzy z jego zakresu, oraz jak te błędy
wyeliminować przed rozpoczęciem korzystania z tak nie do końca poprawnie przygotowanego do pracy
dysku twardego&lt;/p&gt;</description>
    </item>
    
    <item>
	  <author>Mikhail Morfikov</author>
      <title>Aktualizacja firmware drukarki HP LaserJet P2055dn pod linux</title>
      <link>https://morfikov.github.io/post/aktualizacja-firmware-drukarki-hp-laserjet-p2055dn-pod-linux/</link>
      <pubDate>Wed, 27 Jan 2021 19:10:00 +0100</pubDate>
      
      <guid>https://morfikov.github.io/post/aktualizacja-firmware-drukarki-hp-laserjet-p2055dn-pod-linux/</guid>
      <description>&lt;p&gt;Bawiąc się ostatnio drukarką laserową Hewlett Packard (HP) LaserJet P2055dn, zauważyłem, że ma ona
wgrany dość stary firmware. Naturalnie sama drukarka do najmłodszych nie należy, bo została
wyprodukowana w 2011 roku ale skoro na stronie producenta jest dostępna nowsza wersja
oprogramowania dla tego urządzenia, to przydałoby się je do tej drukarki wgrać. Problem jednak
pojawia się w przypadku takich osób jak ja, tj. tych, które korzystają w swoim środowisku pracy z
maszyn mających na pokładzie system operacyjny z rodziny jakieś dystrybucji linux&#39;a, np. Debian czy
Ubuntu. Producent drukarki udostępnia stosowne narzędzia do aktualizacji firmware ale tylko i
wyłącznie dla OS z gatunku Windows. Co mają zrobić osoby, które z Windows&#39;a nie korzystają, a
chciałyby przy tym mieć aktualny firmware w drukarkach HP? Możemy spróbować postawić maszynę
wirtualną na bazie QEMU/KVM, tam zainstalować Windows&#39;a i udostępnić w obrębie tej maszyny
wirtualnej drukarkę, której firmware mamy zamiar aktualizować.&lt;/p&gt;</description>
    </item>
    
    <item>
	  <author>Mikhail Morfikov</author>
      <title>Drukarka laserowa pod linux (HP LaserJet P2055dn)</title>
      <link>https://morfikov.github.io/post/drukarka-laserowa-pod-linux-hp-laserjet-p2055dn/</link>
      <pubDate>Wed, 27 Jan 2021 18:03:00 +0100</pubDate>
      
      <guid>https://morfikov.github.io/post/drukarka-laserowa-pod-linux-hp-laserjet-p2055dn/</guid>
      <description>&lt;p&gt;Parę tygodni temu moja leciwa już (20 letnia) drukarka atramentowa Epson Stylus Color 760
postanowiła odmówić współpracy i zaprzestała dalszego drukowania stron. Po rozkręceniu jej, okazało
się, że ma ona w sobie tyle syfu (m.in. w okolicach dysz), że mechanizmy tej drukarki nie były już
w stanie najwyraźniej tego doczyścić. Pomyślałem, że najwyższy już czas pozbyć się tej drukarki i
poszukać jej następcy. W wytycznych kupna nowej drukarki były przede wszystkim cena (do 500 zł),
bezproblemowa praca pod linux (Debian/Ubuntu) oraz możliwość pracy w sieci (preferowane WiFi) bez
zaprzęgania do tego celu innych sprzętów, np. routera czy komputera stacjonarnego/laptopa. Wybór
padł na poleasingową drukarkę laserową Hewlett Packard (HP) LaserJet P2055dn, ze względu na fakt,
że posiada ona wszystkie te wyżej wymienione właściwości (no może poza WiFi) i można nią zarządzać
bez większego problemu czy to przy pomocy dedykowanych narzędzi z pakietu &lt;code&gt;hplip&lt;/code&gt; / &lt;code&gt;hplip-gui&lt;/code&gt; ,
czy też bezpośrednio za sprawą CUPS.&lt;/p&gt;</description>
    </item>
    
    <item>
	  <author>Mikhail Morfikov</author>
      <title>Zastosowanie KSM w maszynach wirtualnych QEMU/KVM</title>
      <link>https://morfikov.github.io/post/zastosowanie-ksm-w-maszynach-wirtualnych-qemu-kvm/</link>
      <pubDate>Sun, 18 Oct 2020 10:45:00 +0200</pubDate>
      
      <guid>https://morfikov.github.io/post/zastosowanie-ksm-w-maszynach-wirtualnych-qemu-kvm/</guid>
      <description>&lt;p&gt;Użytkownicy linux&#39;a, którzy korzystają z &lt;a href=&#34;https://morfikov.github.io/post/wirtualizacja-qemu-kvm-libvirt-na-debian-linux/&#34;&gt;mechanizmu wirtualizacji QEMU/KVM&lt;/a&gt;, wiedzą, że takie
maszyny wirtualne potrafią zjadać dość sporo pamięci operacyjnej. Im więcej takich maszyn zostanie
uruchomionych w obrębie danego hosta, tym większe ryzyko, że nam tego RAM&#39;u zwyczajnie zabraknie.
Można oczywiście ratować się dokupieniem dodatkowych modułów pamięci ale też nie zawsze taki zabieg
będzie możliwy, zwłaszcza w przypadku domowych stacji roboczych pokroju desktop/laptop. Szukając
rozwiązania tego problemu natrafiłem na coś, co nazywa się Kernel Samepage Merging. W skrócie, KSM
to mechanizm, który ma na celu współdzielenie takich samych stron pamięci operacyjnej przez kilka
procesów. W ten sposób można (przynajmniej teoretycznie) dość znacznie obniżyć zużycie RAM,
zwłaszcza w przypadku korzystania na maszynach wirtualnych z tych samych systemów operacyjnych.
Przydałoby się zatem ocenić jak bardzo KSM wpłynie na wykorzystanie pamięci i czy będzie z niego
jakiś większy użytek zarówno przy korzystaniu z maszyn wirtualnych, czy też w codziennym użytkowaniu
komputera.&lt;/p&gt;</description>
    </item>
    
    <item>
	  <author>Mikhail Morfikov</author>
      <title>Tworzenie kopii zapasowej linux&#39;a z BorgBackup</title>
      <link>https://morfikov.github.io/post/tworzenie-kopii-zapasowej-linux-z-borgbackup/</link>
      <pubDate>Thu, 08 Oct 2020 19:05:00 +0200</pubDate>
      
      <guid>https://morfikov.github.io/post/tworzenie-kopii-zapasowej-linux-z-borgbackup/</guid>
      <description>&lt;p&gt;Gdy chodzi o bezpieczeństwo danych przechowywanych na nośnikach pamięci masowych, takich jak dyski
twarde, to użytkownicy linux&#39;a często piszą sobie skrypty shell&#39;owe mające na celu przeprowadzić
backup całego nośnika lub też jego konkretnych plików/katalogów. Zwykle zaprzęgany jest do pracy
&lt;code&gt;rsync&lt;/code&gt; , który bez problemu jest w stanie  zsynchronizować zawartość dwóch folderów (źródłowego i
docelowego) i po tym procesie wołany jest także &lt;code&gt;tar&lt;/code&gt; mający na celu skompresować pliki backup&#39;u,
tak by zajmowały mniej miejsca. Nie mam nic do tego rozwiązania, bo sam też przez lata z niego
korzystałem ale ma ono całą masę wad. Przede wszystkim, ten mechanizm nie bierze pod uwagę zmian w
samych plikach, czyli tworzy kopię tego co mu się poda i w taki sposób mamy wiele paczek &lt;code&gt;.tar.gz&lt;/code&gt; ,
które zajmują sporo miejsca. Kolejną sprawą jest brak zabezpieczenia przed nieuprawnionym dostępem
do plików kopii zapasowej, np. przy pomocy szyfrowania. W ten sposób trzeba posiłkować się
zewnętrznymi rozwiązaniami, np. pełne szyfrowanie dysku za sprawą LUKS/dm-crypt, co nie zawsze jest
możliwe i też bardzo komplikuje cały proces tworzenia kopii zapasowej, zwłaszcza na zewnętrznych
nośnikach czy zdalnych hostach w sieci. Ostatnio jednak trafiłem na &lt;a href=&#34;https://borgbackup.readthedocs.io/&#34;&gt;narzędzie BorgBackup&lt;/a&gt;, które
to dość znacznie upraszcza cały proces tworzenia backup&#39;u plików na linux, a takie cechy jak
szyfrowanie, kompresja i deduplikacja danych są w borg zaimplementowane standardowo. Postanowiłem
zatem zmigrować z mojego skryptowego systemu tworzenia kopii zapasowych na rzecz borg&#39;a i spisać
przy okazji te użyteczniejsze informacje dotyczące posługiwania się tym narzędziem&lt;/p&gt;</description>
    </item>
    
    <item>
	  <author>Mikhail Morfikov</author>
      <title>Uwierzytelnianie odpowiedzi z serwerów czasu NTP przy pomocy NTS</title>
      <link>https://morfikov.github.io/post/uwierzytelnianie-odpowiedzi-z-serwerow-czasu-ntp-przy-pomocy-nts/</link>
      <pubDate>Sun, 27 Sep 2020 12:29:00 +0200</pubDate>
      
      <guid>https://morfikov.github.io/post/uwierzytelnianie-odpowiedzi-z-serwerow-czasu-ntp-przy-pomocy-nts/</guid>
      <description>&lt;p&gt;Przepisując ostatnio stare artykuły dotyczące &lt;a href=&#34;https://morfikov.github.io/post/szyfrowany-dns-z-dnscrypt-proxy-i-dnsmasq-na-debian-linux/&#34;&gt;zabezpieczenia zapytań DNS za sprawą wdrożenia na
linux dnsmasq i dnscrypt-proxy&lt;/a&gt;, natknąłem się &lt;a href=&#34;https://blog.cloudflare.com/secure-time/&#34;&gt;na informację&lt;/a&gt;, że nie tylko komunikacja DNS
w obecnych czasach w sporej mierze nie jest zaszyfrowana. W zasadzie każda maszyna podłączona do
internetu potrzebuje dysponować w miarę dokładnym czasem. By ten czas był dokładny, wymyślono
mechanizm synchronizacji czasu przez wysyłanie zapytań do serwerów NTP (&lt;a href=&#34;https://pl.wikipedia.org/wiki/Network_Time_Protocol&#34;&gt;Network Time Protocol&lt;/a&gt;).
Niemniej jednak, odpowiedzi z tych serwerów czasu nie są w żaden sposób zabezpieczone i praktycznie
każdy na drodze tych pakietów może nam zmienić ustawienia czasu w systemie (MITM). W taki sposób
możemy zostać cofnięci w czasie, co z kolei może oznaczać, że system zaakceptuje certyfikaty SSL/TLS
czy też &lt;a href=&#34;https://trimstray.github.io/posts/2019-07-21-nginx-optymalizacja_sesji_ssl-tls/#ssl_session_tickets&#34;&gt;klucze/bilety sesji&lt;/a&gt; (używane do wznawiania sesji TLS), które już dawno temu wygasły
lub/i zostały w jakiś sposób skompromitowane. Pchnięcie nas do przodu w czasie również oznacza
problemy, bo możemy zaakceptować certyfikat, który jeszcze nie zaczął być ważny. To z kolei otwiera
drogę do odszyfrowania połączeń z serwisami WWW, a przecie nie po to szyfrujemy ruch, by go ktoś bez
większego problemu odszyfrował. Dlatego też powinniśmy zadbać o to, by informacje o aktualnym czasie
otrzymywane z sieci docierały do nas z wiarygodnego źródła i były w jakiś sposób uwierzytelnione.
Na Debianie standardowo do synchronizacji czasu wykorzystywany jest &lt;code&gt;systemd-timesyncd&lt;/code&gt; ale &lt;a href=&#34;https://github.com/systemd/systemd/issues/9481&#34;&gt;nie
wspiera on póki co protokołu NTS&lt;/a&gt;. Trzeba będzie zatem się go pozbyć i zastąpić go demonem &lt;code&gt;ntpd&lt;/code&gt;
z pakietu &lt;code&gt;ntpsec&lt;/code&gt; .&lt;/p&gt;</description>
    </item>
    
    <item>
	  <author>Mikhail Morfikov</author>
      <title>Szyfrowany DNS z dnscrypt-proxy i dnsmasq na Debian linux</title>
      <link>https://morfikov.github.io/post/szyfrowany-dns-z-dnscrypt-proxy-i-dnsmasq-na-debian-linux/</link>
      <pubDate>Sat, 19 Sep 2020 14:13:00 +0200</pubDate>
      
      <guid>https://morfikov.github.io/post/szyfrowany-dns-z-dnscrypt-proxy-i-dnsmasq-na-debian-linux/</guid>
      <description>&lt;p&gt;Ostatnio na forum dug.net.pl jeden z użytkowników &lt;a href=&#34;https://forum.dug.net.pl/viewtopic.php?id=31524&#34;&gt;miał dość spory problem&lt;/a&gt; z ogarnięciem
zadania polegającego na zaszyfrowaniu zapytań DNS z wykorzystaniem &lt;a href=&#34;https://dnscrypt.info/&#34;&gt;dnscrypt-proxy&lt;/a&gt; i
&lt;a href=&#34;http://www.thekelleys.org.uk/dnsmasq/doc.html&#34;&gt;dnsmasq&lt;/a&gt;. Ładnych parę lat temu opisywałem &lt;a href=&#34;https://morfikov.github.io/tags/resolver/&#34;&gt;jak skonfigurować te dwa narzędzia na Debianie&lt;/a&gt;
(i też na OpenWRT), choć od tamtego czasu w świecie linux&#39;owym trochę rzeczy się pozmieniało. Dla
przykładu, dnscrypt-proxy przeszedł gruntowną przebudowę, no i też systemd jest w powszechniejszym
użyciu niż to miało miejsce w tamtych czasach, przez co w sporej części przypadków usługi takie jak
&lt;code&gt;systemd-networkd.service&lt;/code&gt; czy &lt;code&gt;systemd-resolved.service&lt;/code&gt; są już włączone domyślnie. Zatem sporo
informacji zawartych w tych napisanych przeze mnie artykułach już niekoniecznie może znaleźć
obecnie zastosowanie. Dlatego też pomyślałem, że nadszedł już czas, by ździebko zaktualizować tamte
wpisy. Ostatecznie stanęło jednak na tym, by w oparciu o te artykuły napisać kompletnie nowy tekst
na temat szyfrowania zapytań DNS na linux przy wykorzystaniu oprogramowania &lt;code&gt;dnscrypt-proxy&lt;/code&gt; oraz
&lt;code&gt;dnsmasq&lt;/code&gt; i zawrzeć w nim te wszystkie ciekawsze informacje, które udało mi się pozyskać przez te
ostatnie lata w kwestii poprawy bezpieczeństwa i prywatności przy przeglądaniu stron WWW.&lt;/p&gt;</description>
    </item>
    
    <item>
	  <author>Mikhail Morfikov</author>
      <title>Systemowy GPG/GnuPG w Thunderbird 78&#43; na linux</title>
      <link>https://morfikov.github.io/post/systemowy-gpg-gnupg-w-thunderbird-78-na-linux/</link>
      <pubDate>Mon, 07 Sep 2020 20:15:00 +0200</pubDate>
      
      <guid>https://morfikov.github.io/post/systemowy-gpg-gnupg-w-thunderbird-78-na-linux/</guid>
      <description>&lt;p&gt;Jakiś już czas temu Mozilla ogłosiła, że Thunderbird od wersji 78 będzie posiadał natywne wsparcie
dla szyfrowania wiadomości z wykorzystaniem kluczy GPG/PGP, przez co &lt;a href=&#34;https://enigmail.net/index.php/en/&#34;&gt;dodatek Enigmail&lt;/a&gt; będzie
już zwyczajnie zbędny. Dziś z kolei czytam sobie o &lt;a href=&#34;https://blog.thunderbird.net/2020/09/openpgp-in-thunderbird-78/&#34;&gt;zakończeniu wsparcia dla wersji 68&lt;/a&gt; tego
klienta pocztowego, które będzie miało miejsce z końcem września 2020, czyli został już niespełna
miesiąc i ta starsza wersja Thunderbird&#39;a nie będzie już dostawać łat bezpieczeństwa. Pewne jest
zatem, że dystrybucje linux&#39;a w niedługim czasie pchną wersję 78 do głównych repozytoriów. W
Debianie, wersja 78 Thunderbird&#39;a od dłuższego czasu jest dostępna w gałęzi eksperymentalnej i można
było ją już wcześniej sobie zainstalować, jeśli ktoś wyrażał taką chęć. Gdy ja ostatni raz
testowałem wersję 78, to nie była ona zbytnio do użytku ale wygląda na to, że większość
niedogodności, których mi się udało doświadczyć, została już wyeliminowana. Pozostał w zasadzie
jeden problem, tj. Thunderbird domyślnie używa własnego keyring&#39;a kluczy GPG/PGP, w efekcie czego
systemowy GPG/GnuPG nie jest w ogóle wykorzystywany. Taki san rzeczy sprawia, że będziemy mieć dwie
różne bazy danych kluczy (jedna dla Thunderbird&#39;a, a druga dla reszty linux&#39;a), co może trochę
irytować. Na szczęście jest opcja wymuszenia na TB, by korzystał on z systemowego keyring&#39;a kluczy
GPG/PGP i celem tego artykułu jest pokazanie właśnie jak tego typu zabieg przeprowadzić.&lt;/p&gt;</description>
    </item>
    
    <item>
	  <author>Mikhail Morfikov</author>
      <title>Jak zmusić jeden proces do korzystania z VPN na linux (OpenVPN)</title>
      <link>https://morfikov.github.io/post/jak-zmusic-jeden-proces-do-korzystania-z-vpn-na-linux-openvpn/</link>
      <pubDate>Wed, 02 Sep 2020 18:36:00 +0200</pubDate>
      
      <guid>https://morfikov.github.io/post/jak-zmusic-jeden-proces-do-korzystania-z-vpn-na-linux-openvpn/</guid>
      <description>&lt;p&gt;Parę dni temu &lt;a href=&#34;https://forum.dug.net.pl/viewtopic.php?id=31514&#34;&gt;na forum dug.net.pl pojawiło się zapytanie&lt;/a&gt; dotyczące skonfigurowania linux&#39;a w
taki sposób, by ten umożliwił pojedynczemu procesowi w systemie (i tylko jemu) korzystanie z VPN,
podczas gdy wszystkie pozostałe aplikacje korzystają ze standardowego łącza internetowego naszego
ISP. Oczywiście to niekoniecznie musi być tylko jeden proces, bo to zagadnienie można rozciągnąć
też na większą grupę procesów jednego lub więcej użytkowników. By to zadanie zrealizować, trzeba
zdać sobie sprawę z faktu, że każdy proces w linux ma swojego właściciela, a ten właściciel
przynależy do co najmniej jednej grupy. Dzięki takiemu rozwiązaniu, każdy proces w systemie ma
przypisany m.in. identyfikator użytkownika (UID) oraz identyfikatory grup (GID), z którymi działa i
na podstawie których to linux przyznaje uprawienia dostępu do różnych części systemu, np. urządzeń
czy plików na dysku. W ten sposób możemy bardzo prosto ograniczyć dostęp do określonych zasobów
konkretnym użytkownikom (bardziej ich procesom). Problem się zaczyna w przypadku sieci, gdzie w
zasadzie dostęp do internetu ma domyślnie przyznany każdy proces. Naturalnie możemy skonfigurować
filtr pakietów i zezwolić tylko części aplikacji na dostęp do sieci ale w dalszym ciągu, gdy tylko
odpalimy VPN (w tym przypadku OpenVPN), to każdy proces mający prawo wysyłać pakiety sieciowe
będzie je przesyłał z automatu przez VPN, jak tylko to połączenie zostanie zestawione. Istnieje
jednak sposób, by nauczyć linux&#39;a aby tylko procesy określonych użytkowników czy grup miały dostęp
do VPN i gdy to połączenie zostanie zerwane, to te procesy nie będą mogły korzystać ze
standardowego łącza internetowego. Trzeba jednak zaprzęgnąć do pracy &lt;code&gt;iptables&lt;/code&gt; / &lt;code&gt;nftables&lt;/code&gt; ,
tablice routingu oraz nieco inaczej skonfigurować klienta OpenVPN.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
